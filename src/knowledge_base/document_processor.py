"""
æ–‡æ¡£å¤„ç†å™¨ - æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„åŠ è½½ã€åˆ†å‰²å’Œé¢„å¤„ç†
"""

import os
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime

from langchain_core.documents import Document
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    Docx2txtLoader,
    UnstructuredMarkdownLoader,
    UnstructuredPowerPointLoader
)
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    TokenTextSplitter
)

from .chunking_strategies import ChunkingStrategyFactory, BaseChunkingStrategy

from config.settings import get_model_config, get_settings


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ - æ”¯æŒå¤šç§åˆ†å—ç­–ç•¥"""
    
    def __init__(self, chunking_strategy: str = "recursive", strategy_params: Dict[str, Any] = None):
        self.settings = get_settings()
        self.model_config = get_model_config()
        self.supported_extensions = {
            '.pdf': PyPDFLoader,
            '.txt': TextLoader,
            '.md': UnstructuredMarkdownLoader,
            '.docx': Docx2txtLoader,
            '.pptx': UnstructuredPowerPointLoader,
            # ä»£ç æ–‡ä»¶æ”¯æŒ
            '.py': TextLoader,
            '.js': TextLoader,
            '.ts': TextLoader,
            '.java': TextLoader,
            '.cpp': TextLoader,
            '.c': TextLoader,
            '.h': TextLoader,
            '.hpp': TextLoader,
            '.go': TextLoader,
            '.rs': TextLoader,
            '.php': TextLoader,
            '.rb': TextLoader,
            '.swift': TextLoader,
            '.kotlin': TextLoader,
            '.scala': TextLoader,
            '.r': TextLoader,
            '.sql': TextLoader,
            '.sh': TextLoader,
            '.bat': TextLoader,
            '.ps1': TextLoader,
            # é…ç½®å’Œæ•°æ®æ–‡ä»¶
            '.json': TextLoader,
            '.xml': TextLoader,
            '.yaml': TextLoader,
            '.yml': TextLoader,
            '.toml': TextLoader,
            '.ini': TextLoader,
            '.cfg': TextLoader,
            '.conf': TextLoader,
            '.csv': TextLoader,
            # æ ‡è®°å’Œæ–‡æ¡£æ–‡ä»¶
            '.rst': TextLoader,
            '.html': TextLoader,
            '.htm': TextLoader,
            '.css': TextLoader,
            '.scss': TextLoader,
            '.sass': TextLoader,
            '.less': TextLoader,
            # å…¶ä»–æ–‡æœ¬æ–‡ä»¶
            '.log': TextLoader,
            '.readme': TextLoader,
            '.license': TextLoader,
            '.gitignore': TextLoader,
            '.env': TextLoader
        }
        
        # åˆå§‹åŒ–åˆ†å—ç­–ç•¥
        self.chunking_strategy_name = chunking_strategy
        self.strategy_params = strategy_params or {}
        
        # åˆ›å»ºåˆ†å—ç­–ç•¥å®ä¾‹
        try:
            self.chunking_strategy = ChunkingStrategyFactory.create_strategy(
                chunking_strategy, **self.strategy_params
            )
        except Exception as e:
            print(f"âš ï¸ åˆ†å—ç­–ç•¥åˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {e}")
            # ä½¿ç”¨é»˜è®¤é€’å½’åˆ†å—ç­–ç•¥ä½œä¸ºå¤‡ç”¨
            splitter_config = self.model_config.get_text_splitter_config()
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=splitter_config["parameters"]["chunk_size"],
                chunk_overlap=splitter_config["parameters"]["chunk_overlap"],
                separators=splitter_config["parameters"]["separators"]
            )
            self.chunking_strategy = None
    
    def get_file_hash(self, file_path: Union[str, Path]) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹æ–‡ä»¶å˜åŒ–"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def is_supported_file(self, file_path: Union[str, Path]) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¯æŒ"""
        file_path = Path(file_path)
        return file_path.suffix.lower() in self.supported_extensions
    
    def load_document(self, file_path: Union[str, Path]) -> List[Document]:
        """åŠ è½½å•ä¸ªæ–‡æ¡£"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not self.is_supported_file(file_path):
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
        
        # è·å–å¯¹åº”çš„åŠ è½½å™¨
        loader_class = self.supported_extensions[file_path.suffix.lower()]
        loader = loader_class(str(file_path))
        
        try:
            documents = loader.load()
            
            # æ·»åŠ å…ƒæ•°æ®
            file_hash = self.get_file_hash(file_path)
            file_stats = file_path.stat()
            
            for doc in documents:
                doc.metadata.update({
                    "source": str(file_path),
                    "filename": file_path.name,
                    "file_type": file_path.suffix.lower(),
                    "file_size": file_stats.st_size,
                    "file_hash": file_hash,
                    "created_time": datetime.fromtimestamp(file_stats.st_ctime).isoformat(),
                    "modified_time": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                    "processed_time": datetime.now().isoformat()
                })
            
            return documents
            
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path}: {str(e)}")
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ä½¿ç”¨é…ç½®çš„åˆ†å—ç­–ç•¥åˆ†å‰²æ–‡æ¡£"""
        try:
            if self.chunking_strategy:
                # ä½¿ç”¨ç°ä»£åŒ–çš„åˆ†å—ç­–ç•¥
                split_docs = self.chunking_strategy.chunk_documents(documents)
                
                # æ·»åŠ ç­–ç•¥ä¿¡æ¯åˆ°å…ƒæ•°æ®
                strategy_info = self.chunking_strategy.get_strategy_info()
                for doc in split_docs:
                    doc.metadata.update({
                        "split_time": datetime.now().isoformat(),
                        "strategy_name": strategy_info.get("name", "unknown"),
                        "strategy_description": strategy_info.get("description", "")
                    })
                
                return split_docs
            else:
                # ä½¿ç”¨ä¼ ç»Ÿçš„æ–‡æœ¬åˆ†å‰²å™¨ä½œä¸ºå¤‡ç”¨
                split_docs = self.text_splitter.split_documents(documents)
                
                # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
                for i, doc in enumerate(split_docs):
                    doc.metadata.update({
                        "chunk_id": i,
                        "chunk_size": len(doc.page_content),
                        "split_time": datetime.now().isoformat(),
                        "strategy_name": "legacy_recursive",
                        "strategy_description": "ä¼ ç»Ÿé€’å½’åˆ†å—ç­–ç•¥"
                    })
                
                return split_docs
            
        except Exception as e:
            raise RuntimeError(f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {str(e)}")
    
    def process_file(self, file_path: Union[str, Path], chunking_strategy: str = None, strategy_params: Dict[str, Any] = None) -> List[Document]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼šåŠ è½½ + åˆ†å‰²ï¼ˆæ”¯æŒä¸´æ—¶æŒ‡å®šåˆ†å—ç­–ç•¥ï¼‰"""
        documents = self.load_document(file_path)
        
        # å¦‚æœæŒ‡å®šäº†ä¸´æ—¶ç­–ç•¥ï¼Œåˆ›å»ºä¸´æ—¶å¤„ç†å™¨
        if chunking_strategy and chunking_strategy != self.chunking_strategy_name:
            temp_processor = DocumentProcessor(chunking_strategy, strategy_params or {})
            split_documents = temp_processor.split_documents(documents)
        else:
            split_documents = self.split_documents(documents)
        
        return split_documents
    
    def process_directory(self, directory_path: Union[str, Path], 
                         recursive: bool = True, 
                         auto_strategy: bool = True,
                         chunking_strategy: str = None,
                         strategy_params: Dict[str, Any] = None) -> List[Document]:
        """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ”¯æŒæ–‡ä»¶ï¼ˆæ”¯æŒè‡ªåŠ¨ç­–ç•¥é€‰æ‹©å’Œæ ¼å¼ç‰¹å®šå¤„ç†ï¼‰"""
        directory_path = Path(directory_path)
        
        if not directory_path.exists():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        
        if not directory_path.is_dir():
            raise ValueError(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {directory_path}")
        
        all_documents = []
        processed_files = []
        failed_files = []
        strategy_usage = {}
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        if recursive:
            file_pattern = "**/*"
        else:
            file_pattern = "*"
        
        for file_path in directory_path.glob(file_pattern):
            if file_path.is_file() and self.is_supported_file(file_path):
                try:
                    # ç¡®å®šä½¿ç”¨çš„åˆ†å—ç­–ç•¥
                    file_strategy = chunking_strategy
                    file_strategy_params = strategy_params or {}
                    
                    if auto_strategy and not chunking_strategy:
                        # æ ¹æ®æ–‡ä»¶ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥
                        file_ext = file_path.suffix.lower().lstrip('.')
                        file_strategy = ChunkingStrategyFactory.get_recommended_strategy(
                            file_type=file_ext
                        )
                        
                        # ä¸ºæ ¼å¼ç‰¹å®šç­–ç•¥è®¾ç½®å‚æ•°
                        if file_strategy == "format":
                            file_strategy_params = {"format_type": file_ext}
                        elif file_strategy == "code" and file_ext in ["py", "js", "java", "cpp"]:
                            file_strategy_params = {"language": file_ext}
                    
                    # ç»Ÿè®¡ç­–ç•¥ä½¿ç”¨æƒ…å†µ
                    strategy_usage[file_strategy] = strategy_usage.get(file_strategy, 0) + 1
                    
                    # å¤„ç†æ–‡ä»¶
                    documents = self.process_file(file_path, file_strategy, file_strategy_params)
                    all_documents.extend(documents)
                    processed_files.append(str(file_path))
                    
                    # æ˜¾ç¤ºå¤„ç†ç»“æœï¼ŒåŒ…å«ç­–ç•¥ä¿¡æ¯
                    strategy_display = file_strategy
                    if file_strategy == "format" and "format_type" in file_strategy_params:
                        strategy_display += f"({file_strategy_params['format_type']})"
                    elif file_strategy == "code" and "language" in file_strategy_params:
                        strategy_display += f"({file_strategy_params['language']})"
                    
                    print(f"âœ… å¤„ç†æˆåŠŸ: {file_path.name} ({len(documents)} ä¸ªåˆ†å—) [ç­–ç•¥: {strategy_display}]")
                    
                except Exception as e:
                    failed_files.append((str(file_path), str(e)))
                    print(f"âŒ å¤„ç†å¤±è´¥: {file_path.name} - {str(e)}")
        
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"  æˆåŠŸå¤„ç†: {len(processed_files)} ä¸ªæ–‡ä»¶")
        print(f"  å¤±è´¥æ–‡ä»¶: {len(failed_files)} ä¸ªæ–‡ä»¶")
        print(f"  æ€»åˆ†å—æ•°: {len(all_documents)} ä¸ª")
        
        if strategy_usage:
            print(f"\nğŸ§  åˆ†å—ç­–ç•¥ä½¿ç”¨ç»Ÿè®¡:")
            for strategy, count in sorted(strategy_usage.items()):
                print(f"  {strategy}: {count} ä¸ªæ–‡ä»¶")
        
        return all_documents
    
    def extract_metadata_summary(self, documents: List[Document]) -> Dict[str, Any]:
        """æå–æ–‡æ¡£é›†åˆçš„å…ƒæ•°æ®æ‘˜è¦ï¼ˆåŒ…å«åˆ†å—ç­–ç•¥ç»Ÿè®¡ï¼‰"""
        if not documents:
            return {}
        
        file_types = {}
        chunking_strategies = {}
        total_size = 0
        total_chunks = len(documents)
        sources = set()
        
        for doc in documents:
            metadata = doc.metadata
            
            # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
            file_type = metadata.get("file_type", "unknown")
            file_types[file_type] = file_types.get(file_type, 0) + 1
            
            # ç»Ÿè®¡åˆ†å—ç­–ç•¥
            strategy_name = metadata.get("strategy_name", "unknown")
            chunking_strategies[strategy_name] = chunking_strategies.get(strategy_name, 0) + 1
            
            # ç´¯è®¡æ–‡ä»¶å¤§å°
            total_size += metadata.get("file_size", 0)
            
            # æ”¶é›†æºæ–‡ä»¶
            source = metadata.get("source")
            if source:
                sources.add(source)
        
        return {
            "total_documents": len(sources),
            "total_chunks": total_chunks,
            "total_size_bytes": total_size,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "file_types": file_types,
            "chunking_strategies": chunking_strategies,
            "sources": list(sources)
        }
    
    def get_strategy_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰åˆ†å—ç­–ç•¥ä¿¡æ¯"""
        if self.chunking_strategy:
            return self.chunking_strategy.get_strategy_info()
        else:
            return {
                "name": "legacy_recursive",
                "description": "ä¼ ç»Ÿé€’å½’å­—ç¬¦åˆ†å—ç­–ç•¥",
                "parameters": {
                    "chunk_size": getattr(self.text_splitter, 'chunk_size', 'unknown'),
                    "chunk_overlap": getattr(self.text_splitter, 'chunk_overlap', 'unknown')
                }
            }
    
    @staticmethod
    def list_available_strategies() -> Dict[str, Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„åˆ†å—ç­–ç•¥"""
        return ChunkingStrategyFactory.list_strategies()
    
    @staticmethod
    def get_strategy_recommendation(file_type: str = None, use_case: str = None) -> str:
        """è·å–ç­–ç•¥æ¨è"""
        return ChunkingStrategyFactory.get_recommended_strategy(file_type=file_type, use_case=use_case)


class DocumentValidator:
    """æ–‡æ¡£éªŒè¯å™¨"""
    
    @staticmethod
    def validate_document_content(document: Document) -> bool:
        """éªŒè¯æ–‡æ¡£å†…å®¹æ˜¯å¦æœ‰æ•ˆ"""
        if not document.page_content or not document.page_content.strip():
            return False
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        if len(document.page_content.strip()) < 10:
            return False
        
        return True
    
    @staticmethod
    def validate_documents(documents: List[Document]) -> List[Document]:
        """éªŒè¯å¹¶è¿‡æ»¤æœ‰æ•ˆæ–‡æ¡£"""
        valid_documents = []
        
        for doc in documents:
            if DocumentValidator.validate_document_content(doc):
                valid_documents.append(doc)
            else:
                print(f"âš ï¸  è·³è¿‡æ— æ•ˆæ–‡æ¡£å—: {doc.metadata.get('source', 'unknown')}")
        
        return valid_documents


# åˆ›å»ºå…¨å±€æ–‡æ¡£å¤„ç†å™¨å®ä¾‹ï¼ˆä½¿ç”¨é»˜è®¤é€’å½’ç­–ç•¥ï¼‰
document_processor = DocumentProcessor()