"""
æ–‡æ¡£å¤„ç†å™¨ - æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼çš„åŠ è½½ã€åˆ†å‰²å’Œé¢„å¤„ç†
"""

import os
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime

from langchain_core.documents import Document
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    Docx2txtLoader,
    UnstructuredMarkdownLoader,
    UnstructuredPowerPointLoader
)
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    TokenTextSplitter
)

from config.settings import get_model_config, get_settings


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.settings = get_settings()
        self.model_config = get_model_config()
        self.supported_extensions = {
            '.pdf': PyPDFLoader,
            '.txt': TextLoader,
            '.md': UnstructuredMarkdownLoader,
            '.docx': Docx2txtLoader,
            '.pptx': UnstructuredPowerPointLoader
        }
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        splitter_config = self.model_config.get_text_splitter_config()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=splitter_config["parameters"]["chunk_size"],
            chunk_overlap=splitter_config["parameters"]["chunk_overlap"],
            separators=splitter_config["parameters"]["separators"]
        )
    
    def get_file_hash(self, file_path: Union[str, Path]) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹æ–‡ä»¶å˜åŒ–"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def is_supported_file(self, file_path: Union[str, Path]) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¯æŒ"""
        file_path = Path(file_path)
        return file_path.suffix.lower() in self.supported_extensions
    
    def load_document(self, file_path: Union[str, Path]) -> List[Document]:
        """åŠ è½½å•ä¸ªæ–‡æ¡£"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not self.is_supported_file(file_path):
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
        
        # è·å–å¯¹åº”çš„åŠ è½½å™¨
        loader_class = self.supported_extensions[file_path.suffix.lower()]
        loader = loader_class(str(file_path))
        
        try:
            documents = loader.load()
            
            # æ·»åŠ å…ƒæ•°æ®
            file_hash = self.get_file_hash(file_path)
            file_stats = file_path.stat()
            
            for doc in documents:
                doc.metadata.update({
                    "source": str(file_path),
                    "filename": file_path.name,
                    "file_type": file_path.suffix.lower(),
                    "file_size": file_stats.st_size,
                    "file_hash": file_hash,
                    "created_time": datetime.fromtimestamp(file_stats.st_ctime).isoformat(),
                    "modified_time": datetime.fromtimestamp(file_stats.st_mtime).isoformat(),
                    "processed_time": datetime.now().isoformat()
                })
            
            return documents
            
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path}: {str(e)}")
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£ä¸ºå°å—"""
        try:
            split_docs = self.text_splitter.split_documents(documents)
            
            # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
            for i, doc in enumerate(split_docs):
                doc.metadata.update({
                    "chunk_id": i,
                    "chunk_size": len(doc.page_content),
                    "split_time": datetime.now().isoformat()
                })
            
            return split_docs
            
        except Exception as e:
            raise RuntimeError(f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {str(e)}")
    
    def process_file(self, file_path: Union[str, Path]) -> List[Document]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼šåŠ è½½ + åˆ†å‰²"""
        documents = self.load_document(file_path)
        split_documents = self.split_documents(documents)
        return split_documents
    
    def process_directory(self, directory_path: Union[str, Path], 
                         recursive: bool = True) -> List[Document]:
        """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ”¯æŒæ–‡ä»¶"""
        directory_path = Path(directory_path)
        
        if not directory_path.exists():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        
        if not directory_path.is_dir():
            raise ValueError(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {directory_path}")
        
        all_documents = []
        processed_files = []
        failed_files = []
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        if recursive:
            file_pattern = "**/*"
        else:
            file_pattern = "*"
        
        for file_path in directory_path.glob(file_pattern):
            if file_path.is_file() and self.is_supported_file(file_path):
                try:
                    documents = self.process_file(file_path)
                    all_documents.extend(documents)
                    processed_files.append(str(file_path))
                    print(f"âœ… å¤„ç†æˆåŠŸ: {file_path.name} ({len(documents)} ä¸ªåˆ†å—)")
                    
                except Exception as e:
                    failed_files.append((str(file_path), str(e)))
                    print(f"âŒ å¤„ç†å¤±è´¥: {file_path.name} - {str(e)}")
        
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"  æˆåŠŸå¤„ç†: {len(processed_files)} ä¸ªæ–‡ä»¶")
        print(f"  å¤±è´¥æ–‡ä»¶: {len(failed_files)} ä¸ªæ–‡ä»¶")
        print(f"  æ€»åˆ†å—æ•°: {len(all_documents)} ä¸ª")
        
        return all_documents
    
    def extract_metadata_summary(self, documents: List[Document]) -> Dict[str, Any]:
        """æå–æ–‡æ¡£é›†åˆçš„å…ƒæ•°æ®æ‘˜è¦"""
        if not documents:
            return {}
        
        file_types = {}
        total_size = 0
        total_chunks = len(documents)
        sources = set()
        
        for doc in documents:
            metadata = doc.metadata
            
            # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
            file_type = metadata.get("file_type", "unknown")
            file_types[file_type] = file_types.get(file_type, 0) + 1
            
            # ç´¯è®¡æ–‡ä»¶å¤§å°
            total_size += metadata.get("file_size", 0)
            
            # æ”¶é›†æºæ–‡ä»¶
            source = metadata.get("source")
            if source:
                sources.add(source)
        
        return {
            "total_documents": len(sources),
            "total_chunks": total_chunks,
            "total_size_bytes": total_size,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "file_types": file_types,
            "sources": list(sources)
        }


class DocumentValidator:
    """æ–‡æ¡£éªŒè¯å™¨"""
    
    @staticmethod
    def validate_document_content(document: Document) -> bool:
        """éªŒè¯æ–‡æ¡£å†…å®¹æ˜¯å¦æœ‰æ•ˆ"""
        if not document.page_content or not document.page_content.strip():
            return False
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        if len(document.page_content.strip()) < 10:
            return False
        
        return True
    
    @staticmethod
    def validate_documents(documents: List[Document]) -> List[Document]:
        """éªŒè¯å¹¶è¿‡æ»¤æœ‰æ•ˆæ–‡æ¡£"""
        valid_documents = []
        
        for doc in documents:
            if DocumentValidator.validate_document_content(doc):
                valid_documents.append(doc)
            else:
                print(f"âš ï¸  è·³è¿‡æ— æ•ˆæ–‡æ¡£å—: {doc.metadata.get('source', 'unknown')}")
        
        return valid_documents


# åˆ›å»ºå…¨å±€æ–‡æ¡£å¤„ç†å™¨å®ä¾‹
document_processor = DocumentProcessor()